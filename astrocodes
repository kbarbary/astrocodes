#!/usr/bin/env python
"""Render an html page of astroonomy software packages.
Data is downloaded and cached as needed."""

import os
import json
import sys
import argparse
from collections import OrderedDict
from pprint import pprint
import glob
from datetime import datetime
import time
import shutil
import re

import requests
from bs4 import BeautifulSoup
from jinja2 import Template

DATA_DIR = "data"
STATIC_DIR = "static"
OUTPUT_DIR = "output"

# PYPI_CLASSIFIER_URL: To get the URL for a given classifier, go to
# PyPI's "browse" page (https://pypi.python.org/pypi?%3Aaction=browse),
# browse to the classifier you want, and copy the URL.
PYPI_MAIN_URL = "https://pypi.python.org/pypi?%3Aaction=index"
PYPI_CLASSIFIER_URL = ("https://pypi.python.org/pypi"
                       "?:action=browse&show=all&c=387")
PYPI_PACKAGE_URL = "https://pypi.python.org/pypi/{0}/json"
PYPI_DIR = os.path.join(DATA_DIR, "pypi")

ASCL_URL = "http://ascl.net/code/json"
ASCL_FILE = os.path.join(DATA_DIR, "ascl.json")

PAGES_DIR = os.path.join(DATA_DIR, "pages")

GITHUB_DIR = os.path.join(DATA_DIR, "github")

BITBUCKET_DIR = os.path.join(DATA_DIR, "bitbucket")

def info(msg):
    """Print a line in blue (assumes we have a color terminal!)"""
    print("\033[1m\033[34m", "INFO:", msg, "\033[0m")

auth_token = None


def get_gh_auth_token():
    """Return the GH_TOKEN environment variable or None if not defined."""
    global auth_token
    if auth_token is None and "GH_TOKEN" in os.environ:
        auth_token = os.environ["GH_TOKEN"]
    return auth_token


def urlretrieve(url, fname):
    """ Retrieve URL and write to a file.

    Parameters
    ----------
    url : str
        Specific URL to get using the 'requests' package.
    fname : str
        Name of file to write URLs to.

    """
    info("fetch " + url)
    r = requests.get(url)
    with open(fname, "w") as f:
        f.write(r.text)


def read_json(fname):
    """ Reads from file into JSON. """
    with open(fname, "r") as f:
        return json.load(f)


def write_json(d, fname):
    """ Writes JSON to file. """
    with open(fname, "w") as f:
        json.dump(d, f)


def parse_pypi_html(html_doc, terms=None, strip_version=False):
    """Parse HTML page with package listing from PyPI.

    Parameters
    ----------
    html_doc : str
        HTML document.

    terms : list of str, optional
        If given, only include a package in results if its description includes
        one or more of these terms.

    strip_version : bool, optional
        If True, expect the package name to include a version number and
        strip it off.

    Returns
    -------
    packages : dict
        Dictionary where keys are package names and values are descriptions.
    """

    soup = BeautifulSoup(html_doc)
    result = {}

    # main table on page has class 'list'. Get table data elements from it.
    for row in soup.select("table[class~=list] tr"):
        cells = row.select("td")

        # Header row has no <td>'s and last row has one <td>. Skip these.
        if len(cells) != 2:
            continue

        # first column has an <a href...> element containing the package name.
        name = cells[0].a.string
        description = cells[1].string
        if strip_version:
            name = name.split('\xa0')[0]

        # crop duplicates
        if name in result:
            continue

        # search terms
        if terms is None:
            result[name] = description
        elif description is not None:
            for term in terms:
                if term in description.lower():
                    result[name] = description
                    break

    return result


def load_pypi(clobber=False):
    """Load 'astronomy' packages listed on PyPI.

    This includes packages with Astronomy in the keywords,
    as well as packages with 'astro' or 'cosmo' in the description.

    Downloaded data is cached.

    Parameters
    ----------
    clobber : bool
        If true, re-fetch data even if cached data exists.

    Returns
    -------
    pkgs : dict
       Dictionary, keyed by the package title.
    """

    if not os.path.exists(PYPI_DIR):
        os.makedirs(PYPI_DIR)

    # Get names and descriptions of all "astronomy" packages.
    fname = os.path.join(PYPI_DIR, "packages.json")
    if not os.path.exists(fname) or clobber:
        info("fetch " + PYPI_CLASSIFIER_URL)
        r = requests.get(PYPI_CLASSIFIER_URL)
        info("parse " + PYPI_CLASSIFIER_URL)
        pkgs = parse_pypi_html(r.text)

        info("fetch " + PYPI_MAIN_URL)
        r = requests.get(PYPI_MAIN_URL)
        info("parse " + PYPI_MAIN_URL)
        pkgs2 = parse_pypi_html(r.text, terms=["astro", "cosmo"],
                                strip_version=True)
        pkgs.update(pkgs2)

        write_json(pkgs, fname)
    else:
        pkgs = read_json(fname)

    # Get and save data for each package
    result = {}
    for name in pkgs:
        url = PYPI_PACKAGE_URL.format(name)
        fname = os.path.join(PYPI_DIR, name + ".json")
        if not os.path.exists(fname) or clobber:
            urlretrieve(url, fname)
        result[name.lower()] = read_json(fname)

    return result


def load_ascl(clobber=False):
    """Load packages from the Astrophysics Source Code Library.

    Downloaded data is cached.

    Parameters
    ----------
    clobber : bool
        If true, re-fetch data even if cached data exists.

    Returns
    -------
    pkgs : dict
       Dictionary, keyed by the package title.
    """

    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)

    if not os.path.exists(ASCL_FILE) or clobber:
        urlretrieve(ASCL_URL, ASCL_FILE)
    pkgs = read_json(ASCL_FILE)

    # re-key packages on the title before returning.
    return {pkg["title"].split(":")[0].lower(): pkg
            for pkg in pkgs.values()}


def load_github(user, repo, clobber=False):
    """Return package metadata fetched from GitHub.

    Parameters
    ----------
    user : str
    repo : str
    clobber : bool
        If true, re-fetch data even if cached data exists.

    Returns
    -------
    dict
    """

    if not os.path.exists(GITHUB_DIR):
        os.makedirs(GITHUB_DIR)

    fname = os.path.join(GITHUB_DIR, "{}_{}.json".format(user, repo))
    if not os.path.exists(fname) or clobber:
        url = "https://api.github.com/repos/{}/{}".format(user, repo)

        # supply access token if available.
        token = get_gh_auth_token()
        params = None if token is None else {"access_token": token}

        info("fetch " + url)
        r = requests.get(url, params=params)
        content = 'null' if r.status_code == 404 else r.text
        with open(fname, 'w') as f:
            f.write(content)

    return read_json(fname)


def load_bitbucket(user, repo, clobber=False):
    """ Return package metadata fetched from BitBucket.

    Parameters
    ----------
    user : str
    repo : str
    clobber : bool
        If true, re-fetch data even if cached data exists.

    Returns
    -------
    dict
    """
    if not os.path.exists(BITBUCKET_DIR):
        os.makedirs(BITBUCKET_DIR)

    fname = os.path.join(BITBUCKET_DIR, "{}_{}.json".format(user, repo))
    if not os.path.exists(fname) or clobber:
        url = ("https://api.bitbucket.org/2.0/repositories/{}/{}"
               .format(user, repo))
        info("fetch " + url)
        r = requests.get(url)
        content = "null" if r.status_code == 403 else r.text
        with open(fname, "w") as f:
            f.write(content)

    return read_json(fname)



def load_html_from_url(url, clobber=False):
    """Return HTML as string from URL. Caches downloaded files.

    Parameters
    ----------
    url : str
    clobber : bool
        If True, re-download, even if cached files are present.
    """

    if not os.path.exists(PAGES_DIR):
        os.makedirs(PAGES_DIR)

    # URLSs have slashes in them, so we can't use `url` as a filename
    # when caching the file. So, replace all slashes and colons with
    # '-' to create a filename where we will cache the page. Note that
    # this mapping isn't *completely* foolproof - there can be name
    # collisions! For example, github.com/user/page-name/ and
    # github.com/user/page/name/ will map to the same filename.
    safe_url = url.replace(':', '-').replace('/', '-')
    fname = os.path.join(PAGES_DIR, safe_url)
    if not os.path.exists(fname) or clobber:
        info("fetch " + url)
        try:
            r = requests.get(url, timeout=10.)
            if r.status_code != requests.codes.ok:
                print(r.status_code)
                r.raise_for_status()
            content = r.text
        except ConnectionRefusedError:
            content = ""
        except requests.exceptions.ConnectionError:
            content = ""
        except requests.exceptions.SSLError:
            content = ""
        except requests.exceptions.Timeout:
            content = ""
            # Try replacing url.
            #if "http://" in url:
            #    url = url.replace('tp://', 'tps://')
            #elif "https://" in url:
            #    url = url.replace('tps://', 'tp://')
            #else:
            #     return None
            #
            #info("fetch " + url)
            #try:
            #    r = requests.get(url)  
        except:
            print("Other URL error raised.")
            content = ""

            # TODO: If one of these errors is raised, remove the url and package
            # from the list.

        with open(fname, "w") as f:
            f.write(content)

    with open(fname, 'r') as f:
        return f.read()


def parse_url_as_repo(url_string):
    """ Check if a given URL string is a GitHub or BitBucket repository.

    Parameters
    ----------
    url_string : str

    Returns
    -------
    None or tuple of str
        None if not a repository, otherwise ``(site, user, repo)``
    """

    # TODO: could also check ftp sites, but this would have a different
    #       purpose (can't query an API for ftp sites!)
    REGEX = r"https?://(www\.|)(github\.com|bitbucket\.org)/([^/]+)/([^/]+)/?\Z"
    m = re.match(REGEX, url_string.lower())
    return None if m is None else m.groups()[1:4]

def is_url_a_repo(url):
    return parse_url_as_repo(url) is not None

def parse_html_for_urls(html_doc):
    """ Parse the sourcepage HTML for URLs, and return a list of all URLs.

    Parameters
    ----------
    html_doc : str
        HTML document.

    Returns
    -------
    list of str
        URLs listed in 'a href' in the HTML document.
    """
    soup = BeautifulSoup(html_doc)
    urls = []

    # Scrape page for 'a href' links
    for link in soup.find_all('a'):

        # You found a link!
        # print("You found a link! %s" % link.get('href'))
        temp_link = link.get('href')

        # Crop duplicates
        if temp_link in urls or temp_link is None:
            continue

        # Append unique link to the URL list
        urls.append(temp_link)
    # print("URLs from this doc:")
    # pprint(urls)
    return urls


def find_repo_url_in_page(url):
    """ Get the URL of the source code repository. Check for SSL errors while
    loading source html. URLs from the source webpage listed on PyPI or ASCL.

    Parameters
    ----------
     source_url : str
        The URL listed as the source or homepage of the software package.

    Returns
    -------
    str
        The URL of the source code's repository. Returns 'None' if no source
        code repository URL is found.
    """

    source_html = load_html_from_url(url)
    urls = parse_html_for_urls(source_html)

    # return first link that looks like a repo.
    for url in urls:
        if is_url_a_repo(url):
            return url

    return None


# TODO: use regex's
_LANG_KEYPHRASES = {"python": "Python",
                    "Python": "Python",
                    "Java": "Java",
                    "java": "Java",
                    "in C ": "C",
                    "in C,": "C",
                    "in C.": "C",
                    "C++": "C++",
                    "IDL": "IDL",
                    "FORTRAN": "Fortran",
                    "Fortran": "Fortran",
                    "fortran": "Fortran"}

def detect_language(s):
    """Stupid detection of programming language from a ASCL description"""

    for keyphrase, lang in _LANG_KEYPHRASES.items():
        if keyphrase in s:
            return lang

    return None


def main(clobber=False):
    """Do everything."""

    # Load PyPI packages
    pypi_pkgs = load_pypi(clobber=clobber)

    # Load ASCL packages
    ascl_pkgs = load_ascl(clobber=clobber)

    # clean empty links in ASCL (there is at least one)
    for pkg in ascl_pkgs.values():
        while '' in pkg["site_list"]:
            pkg["site_list"].remove('')

    # get set of all package names
    names = set(pypi_pkgs)
    names.update(ascl_pkgs)

    # combine packages
    packages = []
    for name in names:
        pkg = {"name": name,
               "urls": []}

        pypi_pkg = pypi_pkgs.get(name, None)
        ascl_pkg = ascl_pkgs.get(name, None)

        if pypi_pkg is not None:
            pkg["description"] = pypi_pkg['info']['summary']
            pkg["language"] = "Python"
            pkg["language_from"] = "because package listed on PyPI"

            # Homepage and docs url just both go into url list.
            if pypi_pkg["info"]["home_page"] is not None:
                pkg["urls"].append(pypi_pkg["info"]["home_page"])
            if pypi_pkg["info"]["docs_url"] is not None:
                pkg["urls"].append(pypi_pkg["info"]["docs_url"])

            # Page on pypi.python.org/pypi/
            pkg["pypi_url"] = pypi_pkg['info']['package_url']

        if ascl_pkg is not None:
            pkg["description"] = ascl_pkg["title"].split(':')[-1]
            if "language" not in pkg:
                lang = detect_language(ascl_pkg["abstract"])
                if lang is not None:
                    pkg["language"] = lang
                    pkg["language_from"] = "based on ASCL abstract"
            pkg["urls"].extend(ascl_pkg["site_list"])
            pkg["ascl_url"] = 'http://ascl.net/' + ascl_pkg['ascl_id']

        packages.append(pkg)

    # clean urls:
    # 1. duplicate URLS
    # 2. "UNKNOWN" (appears)
    # 3. "" (appears)
    # 3. ftp sites (not supported by requests) (TODO)
    for pkg in packages:
        pkg["urls"] = list(set(pkg["urls"]))
        pkg["urls"] = list(filter(lambda x: x != "UNKNOWN", pkg["urls"]))
        pkg["urls"] = list(filter(lambda x: x[:6] != "ftp://", pkg["urls"]))
        pkg["urls"] = list(filter(lambda x: x != "", pkg["urls"]))

    # For each package, go through URLs and find ones that look like repos.
    for pkg in packages:
        for i, url in enumerate(pkg["urls"]):
            # marker = "*" if is_url_a_repo(url) else " "
            # print("{:20s} {} {}".format(pkg["name"][:20], marker, url))

            if is_url_a_repo(url):
                # TODO: check multiple repos, as below!
                #if "repo_url" in pkg:
                #    raise RuntimeError("package {:s} has multiple repo URLs!"
                #                       .format(pkg['name']))
                pkg["repo_url"] = pkg["urls"].pop(i)

    # Count packages with a repo
    print("{}/{} software packages with a repo".format(
        sum("repo_url" in pkg for pkg in packages), len(packages)))

    # Attempt to find repo url in home page
    for pkg in packages:
        if "repo_url" not in pkg:
            for url in pkg["urls"]:
                # check that page has .html or .htm or no ending.
                page = url.split("/")[-1]
                if "." not in page or page.split(".")[-1] in ["html", "htm"]:
                    repo_url = find_repo_url_in_page(url)
                    if repo_url is not None:
                        pkg["repo_url"] = repo_url

    # report packages with a repo
    print("{}/{} software packages with a repo".format(
        sum("repo_url" in pkg for pkg in packages), len(packages)))


    # Fetch repo information (github or bitbucket)
    for pkg in packages:
        if "repo_url" in pkg:
            site, user, repo = parse_url_as_repo(pkg["repo_url"])
            
            if site == "github.com":
                github_data = load_github(user, repo, clobber=clobber)
                # pprint(github_data)
                # print("{}/{}".format(user, repo))
                if github_data is None:
                    del pkg["repo_url"]
                else:
                    try:
                        if "language" not in pkg:
                            pkg["language"] = github_data["language"]
                        pkg["forks_count"] = github_data["forks_count"]
                        pkg['github'] = {
                            "stargazers_count": github_data["stargazers_count"],
                            "subscribe_count": github_data["subscribers_count"],
                            "watchers_count": github_data["watchers_count"]
                        }
                        # TODO: get pushed_at and updated_at, use whichever is more
						# recent. Sort list of strings? Convert ISO fmt string to
						# python datetime obj?
						# t = [github_data["pushed_at"], github_data["updated_at"]]
                        pkg["last_updated"] = github_data["pushed_at"]
                        pkg["created_on"] = github_data["created_at"]
                        # exit()
                    except KeyError:
                        del pkg["repo_url"]
						
            elif site == "bitbucket.org":
                bitbucket_data = load_bitbucket(user, repo, clobber=clobber)
                # pprint(bitbucket_data)
                if bitbucket_data is None:
                    del pkg["repo_url"]
                else:
                    try:
                        if "language" not in pkg:
                            pkg["language"] = bitbucket_data["language"]
                        pkg["created_on"] = bitbucket_data["created_on"]
                        pkg["last_updated"] = bitbucket_data["updated_on"]
                        # TODO: get number of watchers (in links:watchers:size),
                        #  Get number of forks (links:forks:size), 
                        # put in pkg["bitbucket"]= {}
                        # exit()
                    except KeyError:
                        del pkg["repo_url"]


    # exit()
    # TODO: add created_on, last_updated, watchers, forks to the printed stuff!

    # sort by name
    packages.sort(key=lambda x: x["name"])

    # load HTML template
    with open(os.path.join(STATIC_DIR, "template.html")) as f:
        template_html = f.read()
    template = Template(template_html)

    # copy static files
    for subdir in ["css", "js", "style"]:
        src = os.path.join(STATIC_DIR, subdir)
        dst = os.path.join(OUTPUT_DIR, subdir)
        if not os.path.exists(dst):
            shutil.copytree(src, dst)

    # render and ouput page
    ctime = time.strftime('%Y %b %d %l:%M %p %Z')
    page = template.render(packages=packages,
                           last_generated=ctime)
    with open(os.path.join(OUTPUT_DIR, "index.html"), "w") as f:
        f.write(page)

    # TODO
    # Link-check ASCL URLs with HEAD requests - cut if all dead.


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--clobber", default=False, action="store_true",
                        help="Refetch all data, even if cached.")
    args = parser.parse_args()
    main(clobber=args.clobber)
